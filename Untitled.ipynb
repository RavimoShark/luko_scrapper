{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scrapper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import logging\n",
    "import re\n",
    "import sys\n",
    "from typing import IO\n",
    "import urllib.error\n",
    "import urllib.parse\n",
    "\n",
    "import aiofiles\n",
    "import aiohttp\n",
    "from aiohttp import ClientSession\n",
    "import pathlib\n",
    "import sys\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)s:%(name)s: %(message)s\",\n",
    "    level=logging.DEBUG,\n",
    "    datefmt=\"%H:%M:%S\",\n",
    "    stream=sys.stderr,\n",
    ")\n",
    "logger = logging.getLogger(\"areq\")\n",
    "logging.getLogger(\"chardet.charsetprober\").disabled = True\n",
    "\n",
    "HREF_RE = re.compile(r'href=\"(.*?)\"')\n",
    "\n",
    "async def fetch_html(url: str, session: ClientSession, **kwargs) -> str:\n",
    "    \"\"\"GET request wrapper to fetch page HTML.\n",
    "\n",
    "    kwargs are passed to `session.request()`.\n",
    "    \"\"\"\n",
    "\n",
    "    resp = await session.request(method=\"GET\", url=url, **kwargs)\n",
    "    resp.raise_for_status()\n",
    "    logger.info(\"Got response [%s] for URL: %s\", resp.status, url)\n",
    "    html = await resp.text()\n",
    "    return html\n",
    "\n",
    "async def parse(url: str, session: ClientSession, **kwargs) -> set:\n",
    "    \"\"\"Find HREFs in the HTML of `url`.\"\"\"\n",
    "    found = set()\n",
    "    try:\n",
    "        html = await fetch_html(url=url, session=session, **kwargs)\n",
    "    except (\n",
    "        aiohttp.ClientError,\n",
    "        aiohttp.http_exceptions.HttpProcessingError,\n",
    "    ) as e:\n",
    "        logger.error(\n",
    "            \"aiohttp exception for %s [%s]: %s\",\n",
    "            url,\n",
    "            getattr(e, \"status\", None),\n",
    "            getattr(e, \"message\", None),\n",
    "        )\n",
    "        return found\n",
    "    except Exception as e:\n",
    "        logger.exception(\n",
    "            \"Non-aiohttp exception occured:  %s\", getattr(e, \"__dict__\", {})\n",
    "        )\n",
    "        return found\n",
    "    else:\n",
    "        for link in HREF_RE.findall(html):\n",
    "            try:\n",
    "                abslink = urllib.parse.urljoin(url, link)\n",
    "            except (urllib.error.URLError, ValueError):\n",
    "                logger.exception(\"Error parsing URL: %s\", link)\n",
    "                pass\n",
    "            else:\n",
    "                found.add(abslink)\n",
    "        logger.info(\"Found %d links for %s\", len(found), url)\n",
    "        return found\n",
    "\n",
    "async def write_one(file: IO, url: str, **kwargs) -> None:\n",
    "    \"\"\"Write the found HREFs from `url` to `file`.\"\"\"\n",
    "    res = await parse(url=url, **kwargs)\n",
    "    if not res:\n",
    "        return None\n",
    "    async with aiofiles.open(file, \"a\") as f:\n",
    "        for p in res:\n",
    "            await f.write(f\"{url}\\t{p}\\n\")\n",
    "        logger.info(\"Wrote results for source URL: %s\", url)\n",
    "\n",
    "async def bulk_crawl_and_write(file: IO, urls: set, **kwargs) -> None:\n",
    "    \"\"\"Crawl & write concurrently to `file` for multiple `urls`.\"\"\"\n",
    "    async with ClientSession() as session:\n",
    "        tasks = []\n",
    "        for url in urls:\n",
    "            tasks.append(\n",
    "                write_one(file=file, url=url, session=session, **kwargs)\n",
    "            )\n",
    "        await asyncio.gather(*tasks)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22:36:08 INFO:areq: Got response [200] for URL: https://1.1.1.1/\n",
      "22:36:08 INFO:areq: Found 13 links for https://1.1.1.1/\n",
      "22:36:08 INFO:areq: Wrote results for source URL: https://1.1.1.1/\n",
      "22:36:08 INFO:areq: Got response [200] for URL: https://www.ietf.org/rfc/rfc2616.txt\n",
      "22:36:08 INFO:areq: Found 0 links for https://www.ietf.org/rfc/rfc2616.txt\n",
      "22:36:08 INFO:areq: Got response [200] for URL: https://regex101.com/\n",
      "22:36:08 ERROR:areq: aiohttp exception for https://docs.python.org/3/this-url-will-404.html [404]: Not Found\n",
      "22:36:08 INFO:areq: Found 24 links for https://regex101.com/\n",
      "22:36:08 INFO:areq: Got response [200] for URL: https://www.mediamatters.org/\n",
      "22:36:08 INFO:areq: Found 109 links for https://www.mediamatters.org/\n",
      "22:36:08 INFO:areq: Wrote results for source URL: https://regex101.com/\n",
      "22:36:08 INFO:areq: Wrote results for source URL: https://www.mediamatters.org/\n",
      "22:36:08 INFO:areq: Got response [200] for URL: https://www.bloomberg.com/markets/economics\n",
      "22:36:08 INFO:areq: Found 3 links for https://www.bloomberg.com/markets/economics\n",
      "22:36:09 INFO:areq: Wrote results for source URL: https://www.bloomberg.com/markets/economics\n",
      "22:36:09 INFO:areq: Got response [200] for URL: https://www.nytimes.com/guides/\n",
      "22:36:09 INFO:areq: Found 125 links for https://www.nytimes.com/guides/\n",
      "22:36:09 INFO:areq: Wrote results for source URL: https://www.nytimes.com/guides/\n",
      "22:36:09 INFO:areq: Got response [200] for URL: https://www.politico.com/tipsheets/morning-money\n",
      "22:36:09 INFO:areq: Found 138 links for https://www.politico.com/tipsheets/morning-money\n",
      "22:36:09 INFO:areq: Wrote results for source URL: https://www.politico.com/tipsheets/morning-money\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "assert sys.version_info >= (3, 7), \"Script requires Python 3.7+.\"\n",
    "here = pathlib.Path('./').parent\n",
    "\n",
    "with open(here.joinpath(\"urls.txt\")) as infile:\n",
    "    urls = set(map(str.strip, infile))\n",
    "\n",
    "outpath = here.joinpath(\"foundurls.txt\")\n",
    "with open(outpath, \"w\") as outfile:\n",
    "    outfile.write(\"source_url\\tparsed_url\\n\")\n",
    "\n",
    "await bulk_crawl_and_write(file=outpath, urls=urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1579820609.106105\n"
     ]
    }
   ],
   "source": [
    "print(datetime.datetime.now().timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ComputerVision",
   "language": "python",
   "name": "computervision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
